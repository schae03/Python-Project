{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42669710-e885-46b9-98c3-c935ab364f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation, Dropout, BatchNormalization   \n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c2952-1abc-4b61-8b44-f69a516386f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=10)       # 0.999\n",
    "np.set_printoptions(suppress=False, precision=10)      # 9.99e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5fb74-a634-433f-81a5-ac02d06112a8",
   "metadata": {},
   "source": [
    "## 다모아져있는거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c31bf-d717-4a89-a199-757352626a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = x <= 0\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "def cross_entropy_error(p, r):\n",
    "    delta = 1e-7\n",
    "    batch_size = p.shape[0]\n",
    "    total_loss = -np.sum(r * np.log(p + delta))\n",
    "    return total_loss / batch_size                               # 평균으로 맞춰줌\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in np.ndindex(x.shape):\n",
    "        tmp_val = x[idx]\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "\n",
    "    return grad\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        c = np.max(x)\n",
    "        exp_a = np.exp(x-c)\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        y = exp_a / sum_exp_a\n",
    "        return y\n",
    "    elif x.ndim == 2:\n",
    "        c = np.max(x, axis = 1).reshape(-1, 1)\n",
    "        exp_a = np.exp(x - c)\n",
    "        sum_exp_a = np.sum(exp_a, axis = 1).reshape(-1, 1)\n",
    "        y = exp_a / sum_exp_a\n",
    "        return y\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout = 1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = x @ self.w + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout @ self.w.T\n",
    "        self.dw = self.x.T @ dout\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cc67c-eb7c-48b8-97d3-626a6f17a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, I, H, O):\n",
    "        self.params = {}\n",
    "        self.params['w1'] = np.random.randn(I, H)\n",
    "        self.params['b1'] = np.random.randn(H)\n",
    "        self.params['w2'] = np.random.randn(H, O)\n",
    "        self.params['b2'] = np.random.randn(O)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['w1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()                       # 시그모이드로 바꾸고 싶으면 이부분만 수정\n",
    "        self.layers['Affine2'] = Affine(self.params['w2'], self.params['b2'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for i in self.layers.values():\n",
    "            x = i.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis = 1)\n",
    "        accuracy = np.sum(y == t) / x.shape[0]\n",
    "        return accuracy\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for i in layers:\n",
    "            dout = i.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['w1'] = self.layers['Affine1'].dw\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['w2'] = self.layers['Affine2'].dw\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128edd0-b1c9-4136-9204-16c66e36f5d7",
   "metadata": {},
   "source": [
    "## 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cc37c-f14f-4cd6-885d-35fd40b9c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= grads[key] * self.lr\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum \n",
    "        self.v = None \n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                # 속도가 모두 0.\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] + grads[key]\n",
    "            params[key] -= self.lr * self.v[key]\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr = 0.01, epsilon = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val) \n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + self.epsilon)\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.iter = 0 \n",
    "        self.m = None\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iter += 1\n",
    "        \n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            \n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "\n",
    "            m_hat = self.m[key] / (1 - self.beta1**self.iter)\n",
    "            v_hat = self.v[key] / (1 - self.beta2**self.iter)\n",
    "            \n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f66844-a24e-488d-a76a-d337907f9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_scaled)\n",
    "batch_size = 100\n",
    "\n",
    "optimizer = AdaGrad()                 # 이거 바꿔가면서\n",
    "\n",
    "train_size = len(train_scaled)\n",
    "batch_size = 100\n",
    "lr = 0.1\n",
    "\n",
    "net = TwoLayerNet(784, 50, 10)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = train_scaled[mask]\n",
    "    t_batch = train_y[mask]\n",
    "\n",
    "    grad = net.gradient(x_batch, t_batch)\n",
    "\n",
    "    params = net.params   \n",
    "    optimizer.update(params, grad)\n",
    "    \n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    test_loss = net.loss(test_scaled, test_y)\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        a = net.accuracy(train_scaled, train_y)\n",
    "        b = net.accuracy(test_scaled, test_y)\n",
    "        train_acc.append(a)\n",
    "        test_acc.append(b)\n",
    "        print(f\"{i}회 학습 / 정확도(훈련) : {a:.3f}, 정확도(시험) : {b:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363d4d5-acad-41ff-9814-744e6ae69496",
   "metadata": {},
   "source": [
    "## loss 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea1a0d-623c-4861-b043-f6d79a19157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label = 'train')\n",
    "plt.plot(test_losses, label = 'test')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbad1a-6c06-41d5-8e80-6a61a3f644a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"])          # 학습\n",
    "plt.plot(history.history[\"val_loss\"])    # 쪽지시험\n",
    "plt.legend([\"train\", \"test\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d91bfd-eeb3-42a6-aa02-bf8b617556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_scaled, test_y)\n",
    "# [loss값, 정확도]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
