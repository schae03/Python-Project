{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62453d8-ac6d-44ea-bf31-c73c27df303f",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "텍스트 마이닝(Text Mining)이란 방대한 양의 비정형(unstructured) 텍스트 데이터 속에서 가치 있는 정보, 패턴, 그리고 인사이트를 추출하고 분석하는 기술입니다. 텍스트 데이터 마이닝(Text Data Mining) 또는 텍스트 분석(Text Analytics)이라고도 불리며, 인공지능(AI), 자연어 처리(NLP), 통계학, 기계 학습 등 다양한 기술을 활용하여 사람이 직접 읽고 파악하기 어려운 대규모 텍스트의 의미를 컴퓨터가 분석할 수 있도록 돕습니다. 쉽게 비유하자면, 텍스트 마이닝은 산더미처럼 쌓인 문서, 이메일, 소셜 미디어 게시글, 뉴스 기사 등에서 금을 캐는 것과 같습니다. 이 과정를 통해 텍스트에 숨겨진 트렌드를 발견하고, 사람들의 감정을 이해하며, 중요한 정보를 요약하는 등 다양한 작업을 수행할 수 있습니다.\n",
    "\n",
    "텍스트 마이닝은 일반적으로 다음과 같은 단계를 거쳐 진행됩니다.\n",
    "\n",
    "1. 데이터 수집 (Data Collection)\n",
    "2. 데이터 전처리 (Data Pre-processing)\n",
    "    * 토큰화 (Tokenization): 문장을 의미 있는 최소 단위인 단어, 형태소 등으로 나누는 과정.\n",
    "    * 정제 (Cleaning): 불필요한 구두점, 특수 문자, HTML 태그 등을 제거.\n",
    "    * 불용어 처리 (Stop-word Removal): '은', '는', '이', '가'와 같이 자주 등장하지만 분석에 큰 의미가 없는 단어(불용어)를 제거.\n",
    "    * 어간 추출 (Stemming) & 표제어 추출 (Lemmatization): 단어의 다양한 변형(예: '달리다', '달리고', '달려서')을 기본형('달리다')으로 통일.\n",
    "3. 텍스트 변환 및 특징 추출 (Text Transformation & Feature Extraction): 전처리된 텍스트를 기계 학습 모델이 처리할 수 있는 숫자 형태의 데이터(벡터)로 변환.\n",
    "4. 텍스트 분석 및 마이닝 (Text Analysis & Mining): 정형화된 데이터를 바탕으로 다양한 분석 기법을 적용하여 패턴과 인사이트를 도출.\n",
    "5. 결과 해석 및 시각화 (Interpretation & Visualization): 분석 결과를 해석하고, 워드 클라우드, 토픽 모델링 시각화, 감성 분석 차트 등 이해하기 쉬운 형태로 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47edefc-ea2c-4b77-a63e-7c2b6c48617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\campus4D019\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\campus4D019\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk    # 사전학습된걸다운\n",
    "\n",
    "# 품사 태깅(Part-of-Speech Tagging, POS Tagging)을 위해 미리 훈련된 모델.\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# # punkt는 마침표나 약어(Mr. , Dr.)와 같은 특별한 언어적 특성을 고려하여 토큰화를 할 수 있게 미리 훈련된 모델.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b81e9-c8bd-4600-95a6-4222eae472ae",
   "metadata": {},
   "source": [
    "# 빈도분석 - 단어단위로 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b3d635-0368-4b1f-89e7-82439b49bac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'reading', 'the', 'comments', 'for', 'this', 'movie', ',', 'I', 'am', 'not', 'sure', 'whether', 'I', 'should', 'be', 'angry', ',', 'sad', 'or', 'sickened', '.', 'Seeing', 'comments', 'typical', 'of', 'people', 'who', 'a', ')', 'know', 'absolutely', 'nothing', 'about', 'the', 'military', 'or', 'b', ')', 'who', 'base', 'everything', 'they', 'think', 'they', 'know', 'on', 'movies', 'like', 'this', 'or', 'on', 'CNN', 'reports', 'about', 'Abu-Gharib', 'makes', 'me', 'wonder', 'about', 'the', 'state', 'of', 'intellectual', 'stimulation', 'in', 'the', 'world', '.', 'At', 'the', 'time', 'I', 'type', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'US', 'military', ':', '1.4', 'million', 'on', 'Active', 'Duty', 'with', 'another', 'almost', '900,000', 'in', 'the', 'Guard', 'and', 'Reserves', 'for', 'a', 'total', 'of', 'roughly', '2.3', 'million', '.', 'The', 'number', 'of', 'people', 'indicted', 'for', 'abuses', 'at', 'at', 'Abu-Gharib', ':', 'Currently', 'less', 'than', '20', 'That', 'makes', 'the', 'total', 'of', 'people', 'indicted', '.', '00083', '%', 'of', 'the', 'total', 'military', '.', 'Even', 'if', 'you', 'indict', 'every', 'single', 'military', 'member', 'that', 'ever', 'stepped', 'in', 'to', 'Abu-Gharib', ',', 'you', 'would', 'not', 'come', 'close', 'to', 'making', 'that', 'a', 'whole', 'number', '.', 'The', 'flaws', 'in', 'this', 'movie', 'would', 'take', 'YEARS', 'to', 'cover', '.', 'I', 'understand', 'that', 'it', \"'s\", 'supposed', 'to', 'be', 'sarcastic', ',', 'but', 'in', 'reality', ',', 'the', 'writer', 'and', 'director', 'are', 'trying', 'to', 'make', 'commentary', 'about', 'the', 'state', 'of', 'the', 'military', 'without', 'an', 'enemy', 'to', 'fight', '.', 'In', 'reality', ',', 'the', 'US', 'military', 'has', 'been', 'at', 'its', 'busiest', 'when', 'there', 'are', 'not', 'conflicts', 'going', 'on', '.', 'The', 'military', 'is', 'the', 'first', 'called', 'for', 'disaster', 'relief', 'and', 'humanitarian', 'aid', 'missions', '.', 'When', 'the', 'tsunami', 'hit', 'Indonesia', ',', 'devestating', 'the', 'region', ',', 'the', 'US', 'military', 'was', 'the', 'first', 'on', 'the', 'scene', '.', 'When', 'the', 'chaos', 'of', 'the', 'situation', 'overwhelmed', 'the', 'local', 'governments', ',', 'it', 'was', 'military', 'leadership', 'who', 'looked', 'at', 'their', 'people', ',', 'the', 'same', 'people', 'this', 'movie', 'mocks', ',', 'and', 'said', 'make', 'it', 'happen', '.', 'Within', 'hours', ',', 'food', 'aid', 'was', 'reaching', 'isolated', 'villages', '.', 'Within', 'days', ',', 'airfields', 'were', 'built', ',', 'cargo', 'aircraft', 'started', 'landing', 'and', 'a', 'food', 'distribution', 'system', 'was', 'up', 'and', 'running', '.', 'Hours', 'and', 'days', ',', 'not', 'weeks', 'and', 'months', '.', 'Yes', 'there', 'are', 'unscrupulous', 'people', 'in', 'the', 'US', 'military', '.', 'But', 'then', ',', 'there', 'are', 'in', 'every', 'walk', 'of', 'life', ',', 'every', 'occupation', '.', 'But', 'to', 'see', 'people', 'on', 'this', 'website', 'decide', 'that', '2.3', 'million', 'men', 'and', 'women', 'are', 'all', 'criminal', ',', 'with', 'nothing', 'on', 'their', 'minds', 'but', 'thoughts', 'of', 'destruction', 'or', 'mayhem', 'is', 'an', 'absolute', 'disservice', 'to', 'the', 'things', 'that', 'they', 'do', 'every', 'day', '.', 'One', 'person', 'on', 'this', 'website', 'even', 'went', 'so', 'far', 'as', 'to', 'say', 'that', 'military', 'members', 'are', 'in', 'it', 'for', 'personal', 'gain', '.', 'Wow', '!', 'Entry', 'level', 'personnel', 'make', 'just', 'under', '$', '8.00', 'an', 'hour', 'assuming', 'a', '40', 'hour', 'work', 'week', '.', 'Of', 'course', ',', 'many', 'work', 'much', 'more', 'than', '40', 'hours', 'a', 'week', 'and', 'those', 'in', 'harm', \"'s\", 'way', 'typically', 'put', 'in', '16-18', 'hour', 'days', 'for', 'months', 'on', 'end', '.', 'That', 'makes', 'the', 'pay', 'well', 'under', 'minimum', 'wage', '.', 'So', 'much', 'for', 'personal', 'gain', '.', 'I', 'beg', 'you', ',', 'please', 'make', 'yourself', 'familiar', 'with', 'the', 'world', 'around', 'you', '.', 'Go', 'to', 'a', 'nearby', 'base', ',', 'get', 'a', 'visitor', 'pass', 'and', 'meet', 'some', 'of', 'the', 'men', 'and', 'women', 'you', 'are', 'so', 'quick', 'to', 'disparage', '.', 'You', 'would', 'be', 'surprised', '.', 'The', 'military', 'no', 'longer', 'accepts', 'people', 'in', 'lieu', 'of', 'prison', 'time', '.', 'They', 'require', 'a', 'minimum', 'of', 'a', 'GED', 'and', 'prefer', 'a', 'high', 'school', 'diploma', '.', 'The', 'middle', 'ranks', 'are', 'expected', 'to', 'get', 'a', 'minimum', 'of', 'undergraduate', 'degrees', 'and', 'the', 'upper', 'ranks', 'are', 'encouraged', 'to', 'get', 'advanced', 'degrees', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\campus4D019\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize          # 단어 토큰화하는 라이브러리\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. \n",
    "Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. \n",
    "At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. \n",
    "The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted. \n",
    "00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. \n",
    "I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. \n",
    "In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. \n",
    "When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. \n",
    "When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. \n",
    "Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\"\"\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(TEXT)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9aac9c3-9ccc-41ec-b842-44e5f7f15ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 30, '.': 29, ',': 21, 'of': 15, 'and': 14, 'to': 13, 'a': 12, 'military': 12, 'in': 12, 'people': 9, 'on': 9, 'are': 9, 'for': 7, 'this': 7, 'that': 6, 'I': 5, 'The': 5, 'you': 5, 'not': 4, 'or': 4, 'about': 4, 'US': 4, 'at': 4, 'every': 4, 'it': 4, 'make': 4, 'was': 4, 'movie': 3, 'be': 3, 'who': 3, 'they': 3, 'Abu-Gharib': 3, 'makes': 3, 'number': 3, 'million': 3, 'with': 3, 'total': 3, 'would': 3, 'an': 3, 'there': 3, 'days': 3, 'hour': 3, 'minimum': 3, 'get': 3, 'comments': 2, ')': 2, 'know': 2, 'nothing': 2, 'base': 2, 'state': 2, 'world': 2, 'time': 2, ':': 2, '2.3': 2, 'indicted': 2, 'than': 2, 'That': 2, \"'s\": 2, 'but': 2, 'reality': 2, 'is': 2, 'first': 2, 'aid': 2, 'When': 2, 'their': 2, 'Within': 2, 'hours': 2, 'food': 2, 'months': 2, 'But': 2, 'website': 2, 'men': 2, 'women': 2, 'so': 2, 'personal': 2, 'gain': 2, 'under': 2, '40': 2, 'work': 2, 'week': 2, 'much': 2, 'ranks': 2, 'degrees': 2, 'After': 1, 'reading': 1, 'am': 1, 'sure': 1, 'whether': 1, 'should': 1, 'angry': 1, 'sad': 1, 'sickened': 1, 'Seeing': 1, 'typical': 1, 'absolutely': 1, 'b': 1, 'everything': 1, 'think': 1, 'movies': 1, 'like': 1, 'CNN': 1, 'reports': 1, 'me': 1, 'wonder': 1, 'intellectual': 1, 'stimulation': 1, 'At': 1, 'type': 1, '1.4': 1, 'Active': 1, 'Duty': 1, 'another': 1, 'almost': 1, '900,000': 1, 'Guard': 1, 'Reserves': 1, 'roughly': 1, 'abuses': 1, 'Currently': 1, 'less': 1, '20': 1, '00083': 1, '%': 1, 'Even': 1, 'if': 1, 'indict': 1, 'single': 1, 'member': 1, 'ever': 1, 'stepped': 1, 'come': 1, 'close': 1, 'making': 1, 'whole': 1, 'flaws': 1, 'take': 1, 'YEARS': 1, 'cover': 1, 'understand': 1, 'supposed': 1, 'sarcastic': 1, 'writer': 1, 'director': 1, 'trying': 1, 'commentary': 1, 'without': 1, 'enemy': 1, 'fight': 1, 'In': 1, 'has': 1, 'been': 1, 'its': 1, 'busiest': 1, 'when': 1, 'conflicts': 1, 'going': 1, 'called': 1, 'disaster': 1, 'relief': 1, 'humanitarian': 1, 'missions': 1, 'tsunami': 1, 'hit': 1, 'Indonesia': 1, 'devestating': 1, 'region': 1, 'scene': 1, 'chaos': 1, 'situation': 1, 'overwhelmed': 1, 'local': 1, 'governments': 1, 'leadership': 1, 'looked': 1, 'same': 1, 'mocks': 1, 'said': 1, 'happen': 1, 'reaching': 1, 'isolated': 1, 'villages': 1, 'airfields': 1, 'were': 1, 'built': 1, 'cargo': 1, 'aircraft': 1, 'started': 1, 'landing': 1, 'distribution': 1, 'system': 1, 'up': 1, 'running': 1, 'Hours': 1, 'weeks': 1, 'Yes': 1, 'unscrupulous': 1, 'then': 1, 'walk': 1, 'life': 1, 'occupation': 1, 'see': 1, 'decide': 1, 'all': 1, 'criminal': 1, 'minds': 1, 'thoughts': 1, 'destruction': 1, 'mayhem': 1, 'absolute': 1, 'disservice': 1, 'things': 1, 'do': 1, 'day': 1, 'One': 1, 'person': 1, 'even': 1, 'went': 1, 'far': 1, 'as': 1, 'say': 1, 'members': 1, 'Wow': 1, '!': 1, 'Entry': 1, 'level': 1, 'personnel': 1, 'just': 1, '$': 1, '8.00': 1, 'assuming': 1, 'Of': 1, 'course': 1, 'many': 1, 'more': 1, 'those': 1, 'harm': 1, 'way': 1, 'typically': 1, 'put': 1, '16-18': 1, 'end': 1, 'pay': 1, 'well': 1, 'wage': 1, 'So': 1, 'beg': 1, 'please': 1, 'yourself': 1, 'familiar': 1, 'around': 1, 'Go': 1, 'nearby': 1, 'visitor': 1, 'pass': 1, 'meet': 1, 'some': 1, 'quick': 1, 'disparage': 1, 'You': 1, 'surprised': 1, 'no': 1, 'longer': 1, 'accepts': 1, 'lieu': 1, 'prison': 1, 'They': 1, 'require': 1, 'GED': 1, 'prefer': 1, 'high': 1, 'school': 1, 'diploma': 1, 'middle': 1, 'expected': 1, 'undergraduate': 1, 'upper': 1, 'encouraged': 1, 'advanced': 1})\n"
     ]
    }
   ],
   "source": [
    "# 각 단어의 출연빈도 확인\n",
    "from collections import Counter\n",
    "vocab = Counter(tokenized_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da650737-fa66-4873-9c6b-7aea5dbea43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'reading', 'comments', 'am', 'sure', 'whether', 'should', 'angry', 'sad', 'sickened', 'Seeing', 'typical', ')', 'know', 'absolutely', 'nothing', 'b', 'base', 'everything', 'think', 'movies', 'like', 'CNN', 'reports', 'me', 'wonder', 'state', 'intellectual', 'stimulation', 'world', 'At', 'time', 'type', ':', '1.4', 'Active', 'Duty', 'another', 'almost', '900,000', 'Guard', 'Reserves', 'roughly', '2.3', 'indicted', 'abuses', 'Currently', 'less', 'than', '20', 'That', '00083', '%', 'Even', 'if', 'indict', 'single', 'member', 'ever', 'stepped', 'come', 'close', 'making', 'whole', 'flaws', 'take', 'YEARS', 'cover', 'understand', \"'s\", 'supposed', 'sarcastic', 'but', 'reality', 'writer', 'director', 'trying', 'commentary', 'without', 'enemy', 'fight', 'In', 'has', 'been', 'its', 'busiest', 'when', 'conflicts', 'going', 'is', 'first', 'called', 'disaster', 'relief', 'humanitarian', 'aid', 'missions', 'When', 'tsunami', 'hit', 'Indonesia', 'devestating', 'region', 'scene', 'chaos', 'situation', 'overwhelmed', 'local', 'governments', 'leadership', 'looked', 'their', 'same', 'mocks', 'said', 'happen', 'Within', 'hours', 'food', 'reaching', 'isolated', 'villages', 'airfields', 'were', 'built', 'cargo', 'aircraft', 'started', 'landing', 'distribution', 'system', 'up', 'running', 'Hours', 'weeks', 'months', 'Yes', 'unscrupulous', 'But', 'then', 'walk', 'life', 'occupation', 'see', 'website', 'decide', 'men', 'women', 'all', 'criminal', 'minds', 'thoughts', 'destruction', 'mayhem', 'absolute', 'disservice', 'things', 'do', 'day', 'One', 'person', 'even', 'went', 'so', 'far', 'as', 'say', 'members', 'personal', 'gain', 'Wow', '!', 'Entry', 'level', 'personnel', 'just', 'under', '$', '8.00', 'assuming', '40', 'work', 'week', 'Of', 'course', 'many', 'much', 'more', 'those', 'harm', 'way', 'typically', 'put', '16-18', 'end', 'pay', 'well', 'wage', 'So', 'beg', 'please', 'yourself', 'familiar', 'around', 'Go', 'nearby', 'visitor', 'pass', 'meet', 'some', 'quick', 'disparage', 'You', 'surprised', 'no', 'longer', 'accepts', 'lieu', 'prison', 'They', 'require', 'GED', 'prefer', 'high', 'school', 'diploma', 'middle', 'ranks', 'expected', 'undergraduate', 'degrees', 'upper', 'encouraged', 'advanced']\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 적은 단어들 추출\n",
    "uncommon_words = []\n",
    "for key, value in vocab.items():            # 단어와 빈도수를 튜플로 볼수잇음\n",
    "    if value <= 2:\n",
    "        uncommon_words.append(key)\n",
    "print(uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c07e5a-f5df-4b23-a708-5ce1b34f9738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',', 'or', '.', 'of', 'people', 'who', 'a', 'about', 'the', 'military', 'or', 'who', 'they', 'they', 'on', 'this', 'or', 'on', 'about', 'Abu-Gharib', 'makes', 'about', 'the', 'of', 'in', 'the', '.', 'the', 'I', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'US', 'military', 'million', 'on', 'with', 'in', 'the', 'and', 'for', 'a', 'total', 'of', 'million', '.', 'The', 'number', 'of', 'people', 'for', 'at', 'at', 'Abu-Gharib', 'makes', 'the', 'total', 'of', 'people', '.', 'of', 'the', 'total', 'military', '.', 'you', 'every', 'military', 'that', 'in', 'to', 'Abu-Gharib', ',', 'you', 'would', 'not', 'to', 'that', 'a', 'number', '.', 'The', 'in', 'this', 'movie', 'would', 'to', '.', 'I', 'that', 'it', 'to', 'be', ',', 'in', ',', 'the', 'and', 'are', 'to', 'make', 'about', 'the', 'of', 'the', 'military', 'an', 'to', '.', ',', 'the', 'US', 'military', 'at', 'there', 'are', 'not', 'on', '.', 'The', 'military', 'the', 'for', 'and', '.', 'the', ',', 'the', ',', 'the', 'US', 'military', 'was', 'the', 'on', 'the', '.', 'the', 'of', 'the', 'the', ',', 'it', 'was', 'military', 'who', 'at', 'people', ',', 'the', 'people', 'this', 'movie', ',', 'and', 'make', 'it', '.', ',', 'was', '.', 'days', ',', ',', 'and', 'a', 'was', 'and', '.', 'and', 'days', ',', 'not', 'and', '.', 'there', 'are', 'people', 'in', 'the', 'US', 'military', '.', ',', 'there', 'are', 'in', 'every', 'of', ',', 'every', '.', 'to', 'people', 'on', 'this', 'that', 'million', 'and', 'are', ',', 'with', 'on', 'of', 'or', 'an', 'to', 'the', 'that', 'they', 'every', '.', 'on', 'this', 'to', 'that', 'military', 'are', 'in', 'it', 'for', '.', 'make', 'an', 'hour', 'a', 'hour', '.', ',', 'a', 'and', 'in', 'in', 'hour', 'days', 'for', 'on', '.', 'makes', 'the', 'minimum', '.', 'for', '.', 'I', 'you', ',', 'make', 'with', 'the', 'you', '.', 'to', 'a', ',', 'get', 'a', 'and', 'of', 'the', 'and', 'you', 'are', 'to', '.', 'would', 'be', '.', 'The', 'military', 'people', 'in', 'of', '.', 'a', 'minimum', 'of', 'a', 'and', 'a', '.', 'The', 'are', 'to', 'get', 'a', 'minimum', 'of', 'and', 'the', 'are', 'to', 'get', '.']\n"
     ]
    }
   ],
   "source": [
    "# 빈도가 많이 나온 단어들만 추출\n",
    "cleaned_by_freq = []\n",
    "for i in tokenized_words:\n",
    "    if i not in uncommon_words:        # 단어가 흔하지 않으면 = 자주 나오는 단어면\n",
    "        cleaned_by_freq.append(i)\n",
    "print(cleaned_by_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8930b02-fcf6-463c-bc47-bca5c1ab79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military', 'who', 'they', 'they', 'this', 'about', 'Abu-Gharib', 'makes', 'about', 'the', 'the', 'the', 'this', 'the', 'number', 'people', 'the', 'military', 'million', 'with', 'the', 'and', 'for', 'total', 'million', 'The', 'number', 'people', 'for', 'Abu-Gharib', 'makes', 'the', 'total', 'people', 'the', 'total', 'military', 'you', 'every', 'military', 'that', 'Abu-Gharib', 'you', 'would', 'not', 'that', 'number', 'The', 'this', 'movie', 'would', 'that', 'the', 'and', 'are', 'make', 'about', 'the', 'the', 'military', 'the', 'military', 'there', 'are', 'not', 'The', 'military', 'the', 'for', 'and', 'the', 'the', 'the', 'military', 'was', 'the', 'the', 'the', 'the', 'the', 'was', 'military', 'who', 'people', 'the', 'people', 'this', 'movie', 'and', 'make', 'was', 'days', 'and', 'was', 'and', 'and', 'days', 'not', 'and', 'there', 'are', 'people', 'the', 'military', 'there', 'are', 'every', 'every', 'people', 'this', 'that', 'million', 'and', 'are', 'with', 'the', 'that', 'they', 'every', 'this', 'that', 'military', 'are', 'for', 'make', 'hour', 'hour', 'and', 'hour', 'days', 'for', 'makes', 'the', 'minimum', 'for', 'you', 'make', 'with', 'the', 'you', 'get', 'and', 'the', 'and', 'you', 'are', 'would', 'The', 'military', 'people', 'minimum', 'and', 'The', 'are', 'get', 'minimum', 'and', 'the', 'are', 'get']\n"
     ]
    }
   ],
   "source": [
    "# 자주 나온 단어들 중 글자수가 3 이상인 단어만 추출\n",
    "cleaned_by_freq_len = []\n",
    "for i in cleaned_by_freq:\n",
    "    if len(i) >= 3:\n",
    "        cleaned_by_freq_len.append(i)\n",
    "print(cleaned_by_freq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177199ba-2a0a-4950-95b7-2f5e91e6e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e84fd0e-9ea3-4af0-9516-e5143846da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도 기준 정제 함수\n",
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    # 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성\n",
    "    vocab = Counter(tokenized_words)\n",
    "    \n",
    "    # 빈도수가 cut_off_count 이하인 단어 set 추출\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    \n",
    "    # uncommon_words에 포함되지 않는 단어 리스트 생성\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "# 단어 길이 기준 정제 함수\n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    # 길이가 cut_off_length 이하인 단어 제거\n",
    "    cleaned_by_freq_len = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_by_freq_len.append(word)\n",
    "\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c9d7905-f865-4282-a98f-137f68f18631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\campus4D019\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 (잘 쓰이지 않는 단어들의 집합) 확인\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ead036a-b1de-4933-b443-64dfa8f3d746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'itself', 'between', 'but', 'while', 'being', 'wouldn', 'him', \"shouldn't\", \"mustn't\", 'themselves', 'd', 'each', 'hadn', 'very', 'they', 'further', 'needn', \"she'll\", \"she's\", 'nor', 'same', 'y', \"you're\", 'haven', \"aren't\", 'are', 'o', 'under', \"wouldn't\", 'these', 'ma', \"i'd\", 'off', 'own', 'am', 'or', 'this', 'a', 'will', 'hasn', \"hasn't\", 'is', 'with', 'yourself', 'herself', \"they've\", 'against', \"we'll\", 'her', 'them', 're', 'by', \"haven't\", 'because', 'ourselves', 'their', 'doesn', 'and', 'whom', 'having', 'than', 'did', 'most', 'at', 'here', 'into', 'you', \"she'd\", 'only', 'the', 'we', 'about', 'aren', 'it', 'won', 'an', 'his', 'again', 'which', 'through', 'now', 'its', 'just', 'both', 'below', 't', 'll', \"we'd\", 'more', 'for', \"it's\", 'm', \"mightn't\", 'all', 'should', \"weren't\", \"shan't\", 'me', \"he'd\", 'your', 'what', \"we're\", 'over', \"i'll\", \"doesn't\", 'above', 'as', 'on', 'why', \"it'll\", \"they'd\", 'ain', \"you've\", \"we've\", 'of', 's', \"wasn't\", 'does', 'no', 'once', 'be', 'after', \"he'll\", 'mightn', \"i've\", \"they'll\", \"couldn't\", \"he's\", 'shan', 'he', \"hadn't\", 'been', 'i', 'do', \"you'll\", 'during', \"isn't\", \"should've\", 'from', 'had', 'himself', 'didn', 'few', 'to', \"i'm\", 'weren', 'can', 'theirs', \"needn't\", \"didn't\", 'mustn', 'in', 'until', 'isn', 'hers', 'have', 'yourselves', 'she', 'doing', 'so', \"they're\", 'if', 'such', 'myself', \"won't\", 'our', 'where', 'those', 'were', \"you'd\", 'then', 'has', 'oh', 'don', \"don't\", 'out', 'when', 'shouldn', 'yours', 'down', 'other', 'was', 'there', 'any', 'too', \"it'd\", 'some', 'who', \"that'll\", 'my', 'that', 've', 'ours', 'how', 'before', 'not', 'wasn', 'up', 'couldn'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 리스트 update로 추가 가능\n",
    "box = [\"oh\", \"the\", \"i\"]\n",
    "stopwords_set.update(box)\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eec368f7-8c9a-45cf-92cf-2403a606904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set.add(\"hello\")          # 불용어 하나는 add로 추가\n",
    "stopwords_set.remove(\"the\")         # 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500cc39-c41a-4d86-8be2-0858ca336859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자주쓰이는 단어 집합 확인\n",
    "cleaned_words = []\n",
    "for i in cleaned_by_freq_len:\n",
    "    if i not in stopwords_set:         # 불용어에 없다면 = 자주쓰이는 단어라면\n",
    "        cleaned_words.append(i)\n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7486aaf0-1a24-49d0-ab8b-411c2f9fc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86852fa3-061f-47d2-bfbb-c1a241c30666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ef38d7-af09-49dc-a7b8-aa712bd38c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'became', 'a', 'USA', 'citizen', '.', 'Ummmmm']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화해서 일반화형태로 변경하는법\n",
    "dic = {\"US\" : \"USA\", \"U.S\" : \"USA\", \"Ummm\" : \"Umm\", \"Ummmm\" : \"Umm\"}\n",
    "\n",
    "text2 = \"She became a US citizen. Ummmmm\"\n",
    "\n",
    "tokenized_words = word_tokenize(text2)        # 토큰화진행\n",
    "\n",
    "normalized_words = []\n",
    "for i in tokenized_words:\n",
    "    if i in dic:           # i가 dic에 있는 단어라면\n",
    "        i = dic[i]         # i의 값 변경\n",
    "    normalized_words.append(i)\n",
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf2e36-13ef-4ffb-bb64-c6603260f43f",
   "metadata": {},
   "source": [
    "# 어간 추출(Stemming)\n",
    "어간 추출은 텍스트 전처리(Text Preprocessing)의 핵심 기술 중 하나로, 단어의 다양한 변형(예: 복수형, 진행형, 과거형 등)을 규칙(rule-based)에 기반하여 단순화된 형태, 즉 '어간(stem)'으로 바꾸는 과정입니다. 예를 들어, studies, studying, studied 와 같은 단어들은 모두 '공부하다'라는 핵심 의미를 공유합니다. 어간 추출은 이 단어들을 모두 studi 라는 공통된 형태로 만들어 단어의 개수를 줄이고 분석의 효율성을 높입니다. 중요한 특징은 어간 추출은 정해진 규칙에 따라 접미사를 잘라내는 방식으로 동작하기 때문에, 결과물이 실제 사전에 존재하는 단어가 아닐 수도 있습니다. (예: studies -> studi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a7e6e4f-6213-42db-b206-715531a56ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f29a59be-9283-4402-be42-74085e388848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program -> program\n",
      "programs -> program\n",
      "programmer -> programm\n",
      "programming -> program\n",
      "programmers -> programm\n",
      "history -> histori\n",
      "historical -> histor\n",
      "computation -> comput\n",
      "computer -> comput\n",
      "compute -> comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  # 어간추출 라이브러리\n",
    "\n",
    "# 어간추출해주는 모델\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# 테스트할 단어 리스트\n",
    "words_to_stem = ['program', 'programs', 'programmer', 'programming', 'programmers']\n",
    "# 비교를 위해 다른 단어들도 추가\n",
    "words_to_stem.extend(['history', 'historical', 'computation', 'computer', 'compute'])\n",
    "\n",
    "for i in words_to_stem:\n",
    "    stem = porter_stemmer.stem(i)\n",
    "    print(f\"{i} -> {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "97e66d2e-1bd3-42d6-ac8a-a177bb42e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포터 스테머 어간 추출 함수\n",
    "def stemming_by_porter(tokenized_words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    porter_stemmed_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        stem = porter_stemmer.stem(word)\n",
    "        porter_stemmed_words.append(stem)\n",
    "    return porter_stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff485602-b702-4d2d-800d-9aff01303176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you', 'are', 'so', 'lovely', '.', 'i', 'am', 'loving', 'you', 'now', '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어간추출 예시\n",
    "\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "text = text.lower()\n",
    "\n",
    "box = []\n",
    "\n",
    "tokenized_words = word_tokenize(text)    # 토큰화 \n",
    "\n",
    "for i in tokenized_words:\n",
    "    stem = porter_stemmer.stem(i)     # 어간추출\n",
    "    box.append(stem)\n",
    "print(box)\n",
    "\n",
    "# lovely와 loving 모두 love로 바뀜\n",
    "\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc79660b-b85f-4b6f-bc99-b582a8c149a4",
   "metadata": {},
   "source": [
    "### 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1de8859-d5da-487f-a767-f3ecb83c3f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jennifer Ehle was sparkling in \\\"Pride and Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  Watching Time Chasers, it obvious that it was ...\n",
       "1  I saw this film about 20 years ago and remembe...\n",
       "2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
       "3  I went to see this film with a great deal of e...\n",
       "4  Yes, I agree with everyone on this site this m...\n",
       "5  Jennifer Ehle was sparkling in \\\"Pride and Pre...\n",
       "6  Amy Poehler is a terrific comedian on Saturday...\n",
       "7  A plane carrying employees of a large biotech ...\n",
       "8  A well made, gritty science fiction movie, it ...\n",
       "9  Incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data/imdb.tsv', sep=\"\\t\")\n",
    "del df['Unnamed: 0']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a20d9a1-2357-406d-af77-632fd2492812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  watching time chasers, it obvious that it was ...\n",
       "1  i saw this film about 20 years ago and remembe...\n",
       "2  minor spoilers in new york, joan barnard (elvi...\n",
       "3  i went to see this film with a great deal of e...\n",
       "4  yes, i agree with everyone on this site this m...\n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6  amy poehler is a terrific comedian on saturday...\n",
       "7  a plane carrying employees of a large biotech ...\n",
       "8  a well made, gritty science fiction movie, it ...\n",
       "9  incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"] = df[\"review\"].str.lower()      # 소문자로 변환\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "789c7eb8-2fbf-4f07-84eb-9caa3cb6cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_token</th>\n",
       "      <th>cleaned_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, the, the, the, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, the, film, the, went, the, jump, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, northam, wonderful, the, the, the, wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, the, author, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, the, ceo, the, the, search, rescue, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, the, the, the, sci-fi, good, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, the, the, the, the]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                          word_token  \\\n",
       "0  [watching, time, chasers, ,, it, obvious, that...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carrying, employees, of, a, large, ...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                       cleaned_token  \n",
       "0  [one, film, said, really, bad, movie, like, sa...  \n",
       "1                        [film, the, the, the, film]  \n",
       "2  [new, york, joan, barnard, elvire, audrey, the...  \n",
       "3  [went, film, the, film, the, went, the, jump, ...  \n",
       "4  [site, movie, bad, even, movie, made, movie, s...  \n",
       "5  [ehle, northam, wonderful, the, the, the, wond...  \n",
       "6  [role, movie, n't, author, book, the, author, ...  \n",
       "7  [plane, the, ceo, the, the, search, rescue, mi...  \n",
       "8  [gritty, movie, the, the, the, sci-fi, good, s...  \n",
       "9                   [girl, girl, the, the, the, the]  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"word_token\"] = df[\"review\"].apply(word_tokenize)      # 토큰화 함수 적용\n",
    "df[\"cleaned_token\"] = df[\"word_token\"].apply(lambda x: clean_by_freq(x, 1))        # 1번 이하로 나온 토큰은 없애겠다\n",
    "df[\"cleaned_token\"] = df[\"cleaned_token\"].apply(lambda x : clean_by_len(x, 2))      # 단어의 길이가 2를 초과하는 것만\n",
    "df[\"cleaned_token\"] = df[\"cleaned_token\"].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf0102ae-b058-44cb-8384-01ae82493a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_token</th>\n",
       "      <th>cleaned_token</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "      <td>[one, film, said, realli, bad, movi, like, sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, the, the, the, film]</td>\n",
       "      <td>[film, the, the, the, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, the...</td>\n",
       "      <td>[new, york, joan, barnard, elvir, audrey, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, the, film, the, went, the, jump, ...</td>\n",
       "      <td>[went, film, the, film, the, went, the, jump, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "      <td>[site, movi, bad, even, movi, made, movi, spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, northam, wonderful, the, the, the, wond...</td>\n",
       "      <td>[ehl, northam, wonder, the, the, the, wonder, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, the, author, ...</td>\n",
       "      <td>[role, movi, n't, author, book, the, author, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, the, ceo, the, the, search, rescue, mi...</td>\n",
       "      <td>[plane, the, ceo, the, the, search, rescu, mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, the, the, the, sci-fi, good, s...</td>\n",
       "      <td>[gritti, movi, the, the, the, sci-fi, good, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, the, the, the, the]</td>\n",
       "      <td>[girl, girl, the, the, the, the]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                          word_token  \\\n",
       "0  [watching, time, chasers, ,, it, obvious, that...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carrying, employees, of, a, large, ...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                       cleaned_token  \\\n",
       "0  [one, film, said, really, bad, movie, like, sa...   \n",
       "1                        [film, the, the, the, film]   \n",
       "2  [new, york, joan, barnard, elvire, audrey, the...   \n",
       "3  [went, film, the, film, the, went, the, jump, ...   \n",
       "4  [site, movie, bad, even, movie, made, movie, s...   \n",
       "5  [ehle, northam, wonderful, the, the, the, wond...   \n",
       "6  [role, movie, n't, author, book, the, author, ...   \n",
       "7  [plane, the, ceo, the, the, search, rescue, mi...   \n",
       "8  [gritty, movie, the, the, the, sci-fi, good, s...   \n",
       "9                   [girl, girl, the, the, the, the]   \n",
       "\n",
       "                                      stemmed_tokens  \n",
       "0  [one, film, said, realli, bad, movi, like, sai...  \n",
       "1                        [film, the, the, the, film]  \n",
       "2  [new, york, joan, barnard, elvir, audrey, the,...  \n",
       "3  [went, film, the, film, the, went, the, jump, ...  \n",
       "4  [site, movi, bad, even, movi, made, movi, spec...  \n",
       "5  [ehl, northam, wonder, the, the, the, wonder, ...  \n",
       "6  [role, movi, n't, author, book, the, author, t...  \n",
       "7  [plane, the, ceo, the, the, search, rescu, mis...  \n",
       "8  [gritti, movi, the, the, the, sci-fi, good, su...  \n",
       "9                   [girl, girl, the, the, the, the]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"stemmed_tokens\"] = df[\"cleaned_token\"].apply(stemming_by_porter)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d178c-b285-4034-b2ed-5177e329f855",
   "metadata": {},
   "source": [
    "# 문장 토큰화(Sentence Tokenization)\n",
    "문장 토큰화는 하나의 긴 텍스트(문서, 단락 등)를 문장의 최소 단위로 분리하는 작업을 말합니다. 즉, 전체 글을 마침표(.), 물음표(?), 느낌표(!) 등을 기준으로 문장 단위의 목록으로 만드는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4af9f331-0f71-401e-97d8-241fa901be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My email address is 'abcde@codeit.com'. Send it to Mr.Kim.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79d61c15-c4b4-48f3-bc33-994b6f39743d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My email address is 'abcde@codeit.com'.\", 'Send it to Mr.Kim.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize      # 문장단위로 쪼개주는 함수\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "841f8b78-7331-4550-941e-c504d4fa1c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you forward my email to Mr.Kim?', 'Thank you!']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Can you forward my email to Mr.Kim? Thank you!\"\n",
    "sent_tokenize(text)   # ?나 ! 있어도 잘 쪼개줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b396756-23a6-451c-99bd-014dd1ef6489",
   "metadata": {},
   "source": [
    "# 품사 태깅(POS Tagging)\n",
    "품사 태깅은 문장 내의 각 단어에 해당하는 품사(명사, 동사, 형용사, 부사 등)를 식별하여 태그를 붙여주는 과정을 말합니다. 이는 컴퓨터가 문장의 문법적 구조를 이해하게 하는 핵심적인 단계입니다. 예를 들어, \"The cat sat on the mat.\" 이라는 문장이 있다면,(The, 관사), (cat, 명사), (sat, 동사), (on, 전치사), (the, 관사), (mat, 명사)\n",
    "\n",
    "* NNP:\t고유명사, 단수\n",
    "* VBZ:\t동사, 3인칭 단수 현재형\n",
    "* DT:\t한정사 (관사 등)\n",
    "* JJ:\t형용사\n",
    "* NN:\t명사, 단수형\n",
    "* IN:\t전치사 또는 접속사\n",
    "* .:\t문장 부호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee3a7d4f-aa78-440e-85e7-e4579c83a9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Watching', 'VBG'),\n",
       " ('Time', 'NNP'),\n",
       " ('Chasers', 'NNPS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('obvious', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('bunch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('friends', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Maybe', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('around', 'IN'),\n",
       " ('one', 'CD'),\n",
       " ('day', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('film', 'NN'),\n",
       " ('school', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('Hey', 'NNP'),\n",
       " (',', ','),\n",
       " ('let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('pool', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('money', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('!', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('Or', 'CC'),\n",
       " ('something', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n",
    "\n",
    "pos_tagged_words = []\n",
    "\n",
    "for sentence in sent_tokenize(text):        # 문장 토큰화\n",
    "    tokenized_words = word_tokenize(sentence)  # 각각의 문장에 대해 단어 토큰화\n",
    "    pos_tagged = pos_tag(tokenized_words)  # 단어 옆에 품사도 함께 구해준다\n",
    "    pos_tagged_words += pos_tagged\n",
    "    \n",
    "pos_tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93d44059-2e36-4ca5-8c5c-2c10e7b31194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 함수\n",
    "def pos_tagger(tokenized_sents):\n",
    "    pos_tagged_words = []\n",
    "\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "    \n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    \n",
    "    return pos_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d06481-3300-4d80-85d1-352766cacc77",
   "metadata": {},
   "source": [
    "# 표제어 추출(Lemmatization) - 원형으로 뽑아줌\n",
    "표제어 추출은 단어의 여러 변형된 형태를 문법적 정보와 ```사전(dictionary)```을 이용하여 그 단어의 기본형, 즉 ```표제어(Lemma)```를 찾아내는 과정입니다. 표제어는 우리가 사전에서 찾는 '기본형 단어'라고 생각하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fecd2223-3ed5-4897-bdb1-436a608f539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\campus4D019\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1b9c5c0c-c7d6-49b6-859a-cb78a4ab0fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is -> is\n",
      "was -> wa\n",
      "are -> are\n",
      "cars -> car\n",
      "dies -> dy\n",
      "flies -> fly\n",
      "watched -> watched\n"
     ]
    }
   ],
   "source": [
    "words_to_lemmatize = ['is', 'was', 'are', 'cars', 'dies', 'flies', 'watched']\n",
    "\n",
    "for i in words_to_lemmatize:\n",
    "    stem = lemmatizer.lemmatize(i)\n",
    "    print(f\"{i} -> {stem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3bcb083b-2de2-4adc-b4c1-b6e60e9f11bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확한 품사를 넣어주는 게 중요\n",
    "\n",
    "lemmatizer.lemmatize(\"dying\", pos = \"v\")   # 동사\n",
    "lemmatizer.lemmatize(\"better\", pos = \"a\")  # 형용사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b25b901c-5a0f-4b0c-9891-27843f1ddba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dog', 'be', 'run', 'and', 'chase', 'the', 'fly', 'bird', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# 품사 별로 정확한 표제어를 찾기 위한 함수\n",
    "def get_wordnet_pos(X):\n",
    "    if X.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif X.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif X.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif X.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "text = \"The dogs are running and chasing the flying birds.\"\n",
    "text = text.lower()\n",
    "\n",
    "word_tokens = word_tokenize(text)    # 단어 토큰화\n",
    "\n",
    "tagged_tokens = pos_tag(word_tokens)    # 단어별로 품사 뽑음\n",
    "\n",
    "lemmatized_words = []\n",
    "for word, tag in tagged_tokens:\n",
    "    pos = get_wordnet_pos(tag)\n",
    "    stem = lemmatizer.lemmatize(word, pos = pos)\n",
    "    lemmatized_words.append(stem)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1224d21-7860-4bd4-99b6-08ffb30a18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출 함수\n",
    "def words_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for word, tag in pos_tagged_words:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "\n",
    "        if wn_tag in (wordnet.NOUN, wordnet.ADJ, wordnet.ADV, wordnet.VERB):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb0892-1c90-4069-a96b-886c137510eb",
   "metadata": {},
   "source": [
    "# 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8d9a9b83-e93e-4e55-b956-bf078a54d7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  watching time chasers, it obvious that it was ...\n",
       "1  i saw this film about 20 years ago and remembe...\n",
       "2  minor spoilers in new york, joan barnard (elvi...\n",
       "3  i went to see this film with a great deal of e...\n",
       "4  yes, i agree with everyone on this site this m...\n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6  amy poehler is a terrific comedian on saturday...\n",
       "7  a plane carrying employees of a large biotech ...\n",
       "8  a well made, gritty science fiction movie, it ...\n",
       "9  incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 문장 토큰화\n",
    "# 2. 품사 태깅\n",
    "# 3. 표제어 추출\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Data/imdb.tsv', sep=\"\\t\")\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "df[\"review\"] = df[\"review\"].str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4ce0333e-db0f-440d-8079-a58c17b98811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watch, time, chaser, ,, it, obvious, that, it...</td>\n",
       "      <td>[make, one, film, say, make, really, bad, movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, year, ago, and...</td>\n",
       "      <td>[film, the, the, the, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoiler, in, new, york, ,, joan, barna...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, go, to, see, this, film, with, a, great, d...</td>\n",
       "      <td>[film, the, film, the, the, jump, send, n't, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, movie, make, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, be, sparkle, in, \\, '', pride...</td>\n",
       "      <td>[ehle, northam, wonderful, the, the, the, wond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, be, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, funny, the, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carry, employee, of, a, large, biot...</td>\n",
       "      <td>[plane, the, ceo, the, the, search, rescue, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, make, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, movie, keep, the, the, the, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, the, the, the, the]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [watching time chasers, it obvious that it was...   \n",
       "1  [i saw this film about 20 years ago and rememb...   \n",
       "2  [minor spoilers in new york, joan barnard (elv...   \n",
       "3  [i went to see this film with a great deal of ...   \n",
       "4  [yes, i agree with everyone on this site this ...   \n",
       "5  [jennifer ehle was sparkling in \\\"pride and pr...   \n",
       "6  [amy poehler is a terrific comedian on saturda...   \n",
       "7  [a plane carrying employees of a large biotech...   \n",
       "8  [a well made, gritty science fiction movie, it...   \n",
       "9  [incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(watching, VBG), (time, NN), (chasers, NNS), ...   \n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...   \n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...   \n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...   \n",
       "4  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...   \n",
       "5  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...   \n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...   \n",
       "7  [(a, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [watch, time, chaser, ,, it, obvious, that, it...   \n",
       "1  [i, saw, this, film, about, 20, year, ago, and...   \n",
       "2  [minor, spoiler, in, new, york, ,, joan, barna...   \n",
       "3  [i, go, to, see, this, film, with, a, great, d...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, be, sparkle, in, \\, '', pride...   \n",
       "6  [amy, poehler, be, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carry, employee, of, a, large, biot...   \n",
       "8  [a, well, make, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [make, one, film, say, make, really, bad, movi...  \n",
       "1                        [film, the, the, the, film]  \n",
       "2  [new, york, joan, barnard, elvire, audrey, the...  \n",
       "3  [film, the, film, the, the, jump, send, n't, j...  \n",
       "4  [site, movie, bad, even, movie, movie, make, m...  \n",
       "5  [ehle, northam, wonderful, the, the, the, wond...  \n",
       "6  [role, movie, n't, author, book, funny, the, a...  \n",
       "7  [plane, the, ceo, the, the, search, rescue, mi...  \n",
       "8  [gritty, movie, movie, keep, the, the, the, sc...  \n",
       "9                   [girl, girl, the, the, the, the]  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sent_tokens\"] = df['review'].apply(sent_tokenize)\n",
    "df[\"pos_tagged_tokens\"] = df[\"sent_tokens\"].apply(pos_tagger)\n",
    "df[\"lemmatized_tokens\"] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "\n",
    "df[\"cleaned_tokens\"] = df['lemmatized_tokens'].apply(lambda x : clean_by_freq(x, 1))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x : clean_by_len(x, 2))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2b52426-46d9-4614-8dee-76daae962b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>combined_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watch, time, chaser, ,, it, obvious, that, it...</td>\n",
       "      <td>[make, one, film, say, make, really, bad, movi...</td>\n",
       "      <td>makeonefilmsaymakereallybadmovielikesaymakerea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, year, ago, and...</td>\n",
       "      <td>[film, the, the, the, film]</td>\n",
       "      <td>filmthethethefilm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoiler, in, new, york, ,, joan, barna...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, the...</td>\n",
       "      <td>newyorkjoanbarnardelvireaudreythebarnardjohnsa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, go, to, see, this, film, with, a, great, d...</td>\n",
       "      <td>[film, the, film, the, the, jump, send, n't, j...</td>\n",
       "      <td>filmthefilmthethejumpsendn'tjumpradion'tsendre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, movie, make, m...</td>\n",
       "      <td>sitemoviebadevenmoviemoviemakemoviespecialdesc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, be, sparkle, in, \\, '', pride...</td>\n",
       "      <td>[ehle, northam, wonderful, the, the, the, wond...</td>\n",
       "      <td>ehlenorthamwonderfulthethethewonderfultheehlen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, be, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, funny, the, a...</td>\n",
       "      <td>rolemovien'tauthorbookfunnytheauthorthetheauth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carry, employee, of, a, large, biot...</td>\n",
       "      <td>[plane, the, ceo, the, the, search, rescue, mi...</td>\n",
       "      <td>planetheceothethesearchrescuemissioncalltheceo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, make, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, movie, keep, the, the, the, sc...</td>\n",
       "      <td>grittymoviemoviekeepthethethesci-figoodkeepsus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, the, the, the, the]</td>\n",
       "      <td>girlgirlthethethethe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [watching time chasers, it obvious that it was...   \n",
       "1  [i saw this film about 20 years ago and rememb...   \n",
       "2  [minor spoilers in new york, joan barnard (elv...   \n",
       "3  [i went to see this film with a great deal of ...   \n",
       "4  [yes, i agree with everyone on this site this ...   \n",
       "5  [jennifer ehle was sparkling in \\\"pride and pr...   \n",
       "6  [amy poehler is a terrific comedian on saturda...   \n",
       "7  [a plane carrying employees of a large biotech...   \n",
       "8  [a well made, gritty science fiction movie, it...   \n",
       "9  [incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(watching, VBG), (time, NN), (chasers, NNS), ...   \n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...   \n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...   \n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...   \n",
       "4  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...   \n",
       "5  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...   \n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...   \n",
       "7  [(a, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [watch, time, chaser, ,, it, obvious, that, it...   \n",
       "1  [i, saw, this, film, about, 20, year, ago, and...   \n",
       "2  [minor, spoiler, in, new, york, ,, joan, barna...   \n",
       "3  [i, go, to, see, this, film, with, a, great, d...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, be, sparkle, in, \\, '', pride...   \n",
       "6  [amy, poehler, be, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carry, employee, of, a, large, biot...   \n",
       "8  [a, well, make, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                      cleaned_tokens  \\\n",
       "0  [make, one, film, say, make, really, bad, movi...   \n",
       "1                        [film, the, the, the, film]   \n",
       "2  [new, york, joan, barnard, elvire, audrey, the...   \n",
       "3  [film, the, film, the, the, jump, send, n't, j...   \n",
       "4  [site, movie, bad, even, movie, movie, make, m...   \n",
       "5  [ehle, northam, wonderful, the, the, the, wond...   \n",
       "6  [role, movie, n't, author, book, funny, the, a...   \n",
       "7  [plane, the, ceo, the, the, search, rescue, mi...   \n",
       "8  [gritty, movie, movie, keep, the, the, the, sc...   \n",
       "9                   [girl, girl, the, the, the, the]   \n",
       "\n",
       "                                     combined_corpus  \n",
       "0  makeonefilmsaymakereallybadmovielikesaymakerea...  \n",
       "1                                  filmthethethefilm  \n",
       "2  newyorkjoanbarnardelvireaudreythebarnardjohnsa...  \n",
       "3  filmthefilmthethejumpsendn'tjumpradion'tsendre...  \n",
       "4  sitemoviebadevenmoviemoviemakemoviespecialdesc...  \n",
       "5  ehlenorthamwonderfulthethethewonderfultheehlen...  \n",
       "6  rolemovien'tauthorbookfunnytheauthorthetheauth...  \n",
       "7  planetheceothethesearchrescuemissioncalltheceo...  \n",
       "8  grittymoviemoviekeepthethethesci-figoodkeepsus...  \n",
       "9                               girlgirlthethethethe  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine(sentence):\n",
    "    return \"\".join(sentence)\n",
    "\n",
    "df[\"combined_corpus\"] = df[\"cleaned_tokens\"].apply(combine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a1d48-d3fc-45d0-af56-6ebb7058770e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c063d8f4-36f8-4c78-9eed-c8cf577f86b6",
   "metadata": {},
   "source": [
    "# 연습문제\n",
    "1. 주어진 영어 문장을 단어 단위로 나누어 리스트 형태로 출력해 보세요. 아래 text 변수에 저장된 문장을 단어로 토큰화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d7845c8a-ea11-4e69-94ef-2ad218c927f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'mining',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'exploring',\n",
       " 'and',\n",
       " 'analyzing',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Text mining is the process of exploring and analyzing large amounts of unstructured text data.\"\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a2398-0d16-4260-87c9-36c53d359f74",
   "metadata": {},
   "source": [
    "2. 주어진 영어 텍스트를 문장 단위로 나누어 리스트 형태로 출력해 보세요. ., ?, ! 등 다양한 문장 부호를 기준으로 문장을 정확히 분리해야 합니다. 아래 text 변수에 저장된 문단을 문장으로 토큰화하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4fbe7c5a-3d4d-46b9-ba1d-bf4a9e1897c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is text mining?',\n",
       " 'It is a fascinating field!',\n",
       " 'We can discover hidden patterns.',\n",
       " 'contact.',\n",
       " 'masterkyungil@gmail.com']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is text mining? It is a fascinating field! We can discover hidden patterns. contact. masterkyungil@gmail.com\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f699d1b-45e1-4b54-9797-93eca622e57f",
   "metadata": {},
   "source": [
    "3. 분석에 큰 의미가 없는 단어들, 즉 불용어를 제거하는 과정입니다. 기본 불용어 목록에 새로운 단어를 추가하여 함께 제거해 보세요. 주어진 단어 리스트에서 NLTK의 영어 불용어를 제거하세요. 추가로 'movie'와 'film'도 불용어로 간주하여 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c33503bb-1052-4dd2-9817-b5849eb08c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'amazing', 'heroic', 'director', '.']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['This', 'is', 'an', 'amazing', 'movie', 'about', 'a', 'heroic', 'film', 'director', '.']\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stopwords_set.add(\"movie\")\n",
    "stopwords_set.add(\"film\")\n",
    "\n",
    "clean_by_stopwords(words, stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c46e6-1e00-405d-b449-887e77bc670e",
   "metadata": {},
   "source": [
    "4. 텍스트에 너무 적게 등장하는 단어는 분석에 유용하지 않을 수 있습니다. 특정 횟수 이하로 등장하는 단어들을 제거해 보세요. 주어진 텍스트에서 1번만 등장하는 단어들을 모두 제거한 결과를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b4514625-7c14-4d49-8483-1ea5b8c99bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cat',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " '.',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " '.',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'dog',\n",
       " '.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The cat sat on the mat. The dog sat on the log. The cat and dog are friends.\"\n",
    "\n",
    "text = text.lower()\n",
    "text = word_tokenize(text)\n",
    "clean_by_freq(text, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a8d52-779f-4325-8c61-d0647a356aa9",
   "metadata": {},
   "source": [
    "5. 단어의 접미사를 규칙에 기반하여 잘라내어 원형에 가깝게 만드는 과정입니다. Porter Stemmer를 사용해 보세요. 주어진 단어 리스트에 포터 스테머(Porter Stemmer)를 적용하여 각 단어의 어간을 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3d125e4f-a51d-42c8-8b7d-fb9ec381f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies -> studi\n",
      "studying -> studi\n",
      "beautiful -> beauti\n",
      "beauty -> beauti\n",
      "connection -> connect\n",
      "connects -> connect\n"
     ]
    }
   ],
   "source": [
    "words = ['studies', 'studying', 'beautiful', 'beauty', 'connection', 'connects']\n",
    "\n",
    "# stemming_by_porter(words)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "for i in words:\n",
    "    stem = porter_stemmer.stem(i)\n",
    "    print(f\"{i} -> {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bcd61c-ded1-4954-88cb-4f7880a0bbfa",
   "metadata": {},
   "source": [
    "6. 단어의 사전적, 문법적 의미를 고려하여 기본형(표제어)을 찾는 과정입니다. 어간 추출과의 차이점을 비교해 보세요. 연습문제 5와 동일한 단어 리스트에 표제어 추출을 적용하세요. pos 인자를 사용하지 않고 명사(기본값) 기준으로 표제어를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1b0fd2ea-36e6-4349-892d-5da8a81b5286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies -> study\n",
      "studying -> studying\n",
      "beautiful -> beautiful\n",
      "beauty -> beauty\n",
      "connection -> connection\n",
      "connects -> connects\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    stem = lemmatizer.lemmatize(i)\n",
    "    print(f\"{i} -> {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b641959-5de5-4c24-80a3-e905e52f5888",
   "metadata": {},
   "source": [
    "7. 장 내 각 단어의 품사를 알아내어 태그를 붙이는 과정입니다. 이는 더 정확한 표제어 추출의 기반이 됩니다. 주어진 문장을 단어로 토큰화한 후, 각 단어에 대한 품사를 태깅하여 (단어, 품사) 튜플의 리스트로 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "38011f30-31cc-471e-b9f4-7d2a9bb11d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'NN'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'NNS'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'NN'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "tokenized_words = word_tokenize(text)\n",
    "pos_tagger(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74f3d1-a4d6-45d0-94d3-c33339bd842c",
   "metadata": {},
   "source": [
    "8. 품사 정보를 함께 사용하면 표제어 추출의 정확도를 크게 높일 수 있습니다. is는 be로, running은 run으로 정확히 변환해 보세요. 주어진 문장에 대해 품사 태깅을 수행하고, 이 품사 정보를 활용하여 표제어를 추출하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e8d91b3c-0d18-4f07-a3d4-2b7662b296d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'be', 'run', 'faster', 'than', 'I', 'thought', '.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"She is running faster than I thought.\"\n",
    "\n",
    "tokenized_words = word_tokenize(text)\n",
    "pos_tagged_words = pos_tagger(tokenized_words)\n",
    "words_lemmatizer(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459f516-ccf0-4452-beac-1a14febd7972",
   "metadata": {},
   "source": [
    "9. 지금까지 배운 여러 전처리 기술을 순서대로 적용하는 함수를 만들어 보세요. 다음 텍스트에 대해 아래의 전처리 과정을 순서대로 적용하는 preprocess_text 함수를 완성하세요.\n",
    "* 모든 알파벳을 소문자로 변환\n",
    "* 단어 토큰화\n",
    "* 길이가 2 이하인 단어 제거\n",
    "* 영어 불용어 제거\n",
    "* 포터 스테머를 이용한 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f113d574-380e-4f82-9910-910dfd301a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'scienc', 'becom', 'one', 'popular', 'field', '21st', 'centuri']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Data science has become one of the most popular fields in the 21st century.\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokenized_words = word_tokenize(text)\n",
    "    \n",
    "    cleaned_words = clean_by_len(tokenized_words, 2)\n",
    "    \n",
    "    stopwords_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_stopwords = clean_by_stopwords(cleaned_words, stopwords_set)\n",
    "    \n",
    "    return stemming_by_porter(cleaned_stopwords)\n",
    "\n",
    "preprocess_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0d9ab-e23c-490b-b77e-fba2acf2a191",
   "metadata": {},
   "source": [
    "10. 실제 데이터 분석에서는 Pandas DataFrame 형태로 텍스트 데이터를 다루는 경우가 많습니다. DataFrame의 각 행에 전처리 함수를 일괄 적용해 보세요. 주어진 데이터로 Pandas DataFrame을 생성한 후, review 열의 각 텍스트에 연습문제 9에서 만든 preprocess_text 함수를 적용하여 processed_review라는 새로운 열을 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fcfcac32-ba6c-4637-94e8-78ae30b511a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was absolutely fantastic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've never seen such a boring film in my life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A truly inspiring and emotional journey.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review\n",
       "0            This movie was absolutely fantastic!\n",
       "1  I've never seen such a boring film in my life.\n",
       "2        A truly inspiring and emotional journey."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'review': [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I've never seen such a boring film in my life.\",\n",
    "    \"A truly inspiring and emotional journey.\"\n",
    "]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b86d2742-48e5-4fa6-884e-75686bb4ad5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was absolutely fantastic!</td>\n",
       "      <td>[This movie was absolutely fantastic!]</td>\n",
       "      <td>[(This, DT), (movie, NN), (was, VBD), (absolut...</td>\n",
       "      <td>[This, movie, be, absolutely, fantastic, !]</td>\n",
       "      <td>[This, movie, absolutely, fantastic]</td>\n",
       "      <td>[movi, absolut, fantast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've never seen such a boring film in my life.</td>\n",
       "      <td>[I've never seen such a boring film in my life.]</td>\n",
       "      <td>[(I, PRP), ('ve, VBP), (never, RB), (seen, VBN...</td>\n",
       "      <td>[I, 've, never, see, such, a, boring, film, in...</td>\n",
       "      <td>['ve, never, see, boring, film, life]</td>\n",
       "      <td>['ve, never, seen, bore, film, life]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A truly inspiring and emotional journey.</td>\n",
       "      <td>[A truly inspiring and emotional journey.]</td>\n",
       "      <td>[(A, DT), (truly, NN), (inspiring, NN), (and, ...</td>\n",
       "      <td>[A, truly, inspiring, and, emotional, journey, .]</td>\n",
       "      <td>[truly, inspiring, emotional, journey]</td>\n",
       "      <td>[truli, inspir, emot, journey]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review  \\\n",
       "0            This movie was absolutely fantastic!   \n",
       "1  I've never seen such a boring film in my life.   \n",
       "2        A truly inspiring and emotional journey.   \n",
       "\n",
       "                                        sent_tokens  \\\n",
       "0            [This movie was absolutely fantastic!]   \n",
       "1  [I've never seen such a boring film in my life.]   \n",
       "2        [A truly inspiring and emotional journey.]   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(This, DT), (movie, NN), (was, VBD), (absolut...   \n",
       "1  [(I, PRP), ('ve, VBP), (never, RB), (seen, VBN...   \n",
       "2  [(A, DT), (truly, NN), (inspiring, NN), (and, ...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0        [This, movie, be, absolutely, fantastic, !]   \n",
       "1  [I, 've, never, see, such, a, boring, film, in...   \n",
       "2  [A, truly, inspiring, and, emotional, journey, .]   \n",
       "\n",
       "                           cleaned_tokens  \\\n",
       "0    [This, movie, absolutely, fantastic]   \n",
       "1   ['ve, never, see, boring, film, life]   \n",
       "2  [truly, inspiring, emotional, journey]   \n",
       "\n",
       "                       processed_review  \n",
       "0              [movi, absolut, fantast]  \n",
       "1  ['ve, never, seen, bore, film, life]  \n",
       "2        [truli, inspir, emot, journey]  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sent_tokens\"] = df['review'].apply(sent_tokenize)\n",
    "df[\"pos_tagged_tokens\"] = df[\"sent_tokens\"].apply(pos_tagger)\n",
    "df[\"lemmatized_tokens\"] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "\n",
    "df[\"cleaned_tokens\"] = df['lemmatized_tokens'].apply(lambda x : clean_by_len(x, 2))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "\n",
    "df[\"processed_review\"] = df[\"review\"].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f3aa9-8823-430e-976e-8d9893c6070e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
